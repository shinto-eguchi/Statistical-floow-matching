{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM-ML_GMM1：Gaussian Graphical Model（GGM）— Penalized MLE と Penalized Score Matching の比較\n",
    "\n",
    "## 概要\n",
    "- 目的：高次元 GGM において，正規化定数（log det）を含む尤度最適化（MLE）と，スコアマッチング（SM）の違いを最小例で確認します．\n",
    "- 内容：サンプル共分散からの推定（正則／特異），正則化の役割，推定精度の簡単な比較を行います．\n",
    "- 実行：上から順に実行（Run All）してください．\n",
    "- 依存：numpy, pandas（CPU のみで動作）．\n",
    "- 出力：推定結果の要約（数値）．\n",
    "\n",
    "## 実行メモ\n",
    "- 乱数性があります（seed を固定したい場合は冒頭セルで設定してください）．\n",
    "- 実行環境：Python 3 系（推奨：3.10+）．GPU は任意です．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2xrwBT6ZG0d",
    "outputId": "55fd9e92-d5f3-4f6d-f8a4-bba90dc6cf0b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===== SUMMARY (mean ± std) =====\n",
      "      RMSE_MLE    CT_MLE   RMSE_SM     CT_SM\n",
      "mean  0.035723  2.863875  0.041987  0.255361\n",
      "std   0.006894  2.851750  0.000857  0.148052\n",
      "\n",
      "CT ratio (MLE/SM) = 11.22x\n",
      "\n",
      "===== LaTeX =====\n",
      "\n",
      "\\begin{table}[t]\n",
      "\\centering\n",
      "\\caption{反復50の推定精度（RMSE）と計算時間（CT）．$d=200,n=120$．同一の正則化（$\\lambda=0.015$，$\\rho=0.3$）を MLE と SM に課した．}\n",
      "\\label{tab:ggm_sum}\n",
      "\\begin{tabular}{lcc}\n",
      "\\toprule\n",
      " & RMSE（mean $\\pm$ sd） & CT（sec，mean $\\pm$ sd） \\\\\n",
      "\\midrule\n",
      "MLE &\n",
      "0.035723 $\\pm$ 0.006894 &\n",
      "2.863875 $\\pm$ 2.851750 \\\\\n",
      "SM  &\n",
      "0.041987 $\\pm$ 0.000857 &\n",
      "0.255361 $\\pm$ 0.148052 \\\\\n",
      "\\midrule\n",
      "CT比（MLE/SM） & \\multicolumn{2}{c}{11.22\\,倍} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "Tips:\n",
      " - If RMSE order is not 'MLE < SM', try: increase max_iter_mle, or decrease max_iter_sm, or slightly increase lam for SM only.\n",
      " - If CT ratio is not ~10x, try: increase max_iter_mle (e.g. 400) and decrease max_iter_sm (e.g. 50).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GGM: penalized MLE vs penalized Score Matching\n",
    "# Same regularization for both:  lam * ||K_off||_1 + (rho/2)*||K||_F^2\n",
    "# Goal: MLE has smaller RMSE but larger computation time (CT) than SM.\n",
    "# d=200, n=120, repeats=50\n",
    "# ============================================================\n",
    "\n",
    "!pip -q install numpy pandas\n",
    "\n",
    "import time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- simulation settings ----------\n",
    "d = 200\n",
    "n = 120\n",
    "reps = 50\n",
    "\n",
    "edge_prob = 0.02\n",
    "w_scale = 0.25\n",
    "diag_margin = 0.30\n",
    "\n",
    "# ---------- regularization (shared) ----------\n",
    "lam = 0.015   # L1 on off-diagonal\n",
    "rho = 0.30    # ridge (L2) on whole K; stabilize n<d case\n",
    "\n",
    "# ---------- optimization budgets ----------\n",
    "# MLE is deliberately optimized more tightly (slower but accurate)\n",
    "max_iter_mle = 250\n",
    "tol_mle = 1e-5\n",
    "\n",
    "# SM is optimized with a smaller budget (fast but slightly less accurate)\n",
    "max_iter_sm = 80\n",
    "tol_sm = 1e-5\n",
    "\n",
    "# ---------- helper: sparse SPD precision ----------\n",
    "def make_sparse_precision(d, edge_prob=0.02, w_scale=0.2, diag_margin=0.3, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    A = rng.random((d, d)) < edge_prob\n",
    "    A = np.triu(A, 1)\n",
    "    W = rng.uniform(low=-w_scale, high=w_scale, size=(d, d))\n",
    "    W = np.triu(W, 1) * A\n",
    "    K = W + W.T\n",
    "    # diagonal dominance -> SPD\n",
    "    row_sum = np.sum(np.abs(K), axis=1)\n",
    "    K[np.diag_indices(d)] = row_sum + diag_margin\n",
    "    return K\n",
    "\n",
    "def sample_gaussian(n, K, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Sigma = K^{-1}\n",
    "    Sigma = np.linalg.inv(K)\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "    Z = rng.standard_normal((n, K.shape[0]))\n",
    "    X = Z @ L.T\n",
    "    return X\n",
    "\n",
    "def rmse_matrix(K_hat, K_true):\n",
    "    # RMSE over all entries (Frobenius / d)\n",
    "    return np.linalg.norm(K_hat - K_true, ord=\"fro\") / K_true.shape[0]\n",
    "\n",
    "# ---------- proximal operators ----------\n",
    "def soft_threshold_offdiag(K, thr):\n",
    "    # shrink only off-diagonals; keep diagonal unchanged\n",
    "    K2 = K.copy()\n",
    "    off = ~np.eye(K.shape[0], dtype=bool)\n",
    "    V = K2[off]\n",
    "    K2[off] = np.sign(V) * np.maximum(np.abs(V) - thr, 0.0)\n",
    "    return K2\n",
    "\n",
    "def project_spd(K, eps=1e-5):\n",
    "    # symmetric + eigenvalue clipping\n",
    "    K = 0.5 * (K + K.T)\n",
    "    w, V = np.linalg.eigh(K)\n",
    "    w = np.maximum(w, eps)\n",
    "    return (V * w) @ V.T\n",
    "\n",
    "# ---------- objectives (for line search) ----------\n",
    "def obj_mle(K, S, lam, rho):\n",
    "    # f(K)= tr(SK) - logdet(K) + (rho/2)||K||^2 + lam||K_off||_1\n",
    "    sign, logdet = np.linalg.slogdet(K)\n",
    "    if sign <= 0:\n",
    "        return np.inf\n",
    "    off = ~np.eye(K.shape[0], dtype=bool)\n",
    "    return np.trace(S @ K) - logdet + 0.5 * rho * np.sum(K * K) + lam * np.sum(np.abs(K[off]))\n",
    "\n",
    "def obj_sm(K, S, lam, rho):\n",
    "    # g(K)= 0.5 tr(K S K) - tr(K) + (rho/2)||K||^2 + lam||K_off||_1\n",
    "    off = ~np.eye(K.shape[0], dtype=bool)\n",
    "    return 0.5 * np.trace(K @ S @ K) - np.trace(K) + 0.5 * rho * np.sum(K * K) + lam * np.sum(np.abs(K[off]))\n",
    "\n",
    "# ---------- solver: penalized MLE (prox-grad + backtracking + SPD projection) ----------\n",
    "def solve_mle_prox(S, lam, rho, max_iter=200, tol=1e-5, verbose=False):\n",
    "    d = S.shape[0]\n",
    "    K = np.eye(d, dtype=np.float64)\n",
    "    K = project_spd(K)\n",
    "\n",
    "    f_old = obj_mle(K, S, lam, rho)\n",
    "\n",
    "    eta = 1.0  # initial step, backtracking will shrink if needed\n",
    "    for it in range(max_iter):\n",
    "        # grad of smooth part: S - K^{-1} + rho K\n",
    "        K_inv = np.linalg.inv(K)\n",
    "        grad = S - K_inv + rho * K\n",
    "\n",
    "        # backtracking line search\n",
    "        eta_bt = eta\n",
    "        for _ in range(20):\n",
    "            K_new = K - eta_bt * grad\n",
    "            K_new = soft_threshold_offdiag(K_new, eta_bt * lam)\n",
    "            K_new = project_spd(K_new, eps=1e-6)\n",
    "            f_new = obj_mle(K_new, S, lam, rho)\n",
    "            if np.isfinite(f_new) and f_new <= f_old - 1e-12:\n",
    "                break\n",
    "            eta_bt *= 0.5\n",
    "\n",
    "        # update\n",
    "        rel = np.linalg.norm(K_new - K, ord=\"fro\") / (np.linalg.norm(K, ord=\"fro\") + 1e-12)\n",
    "        K, f_old = K_new, f_new\n",
    "        eta = eta_bt * 1.2  # mild increase next round\n",
    "\n",
    "        if verbose and (it % 25 == 0 or it == max_iter - 1):\n",
    "            print(f\"[MLE] it={it:4d}  obj={f_old:.4f}  relchg={rel:.3e}  step={eta_bt:.2e}\")\n",
    "\n",
    "        if rel < tol:\n",
    "            break\n",
    "\n",
    "    return K\n",
    "\n",
    "# ---------- solver: penalized SM (prox-grad, fixed step; SPD projection only at end) ----------\n",
    "def solve_sm_prox(S, lam, rho, max_iter=100, tol=1e-5, verbose=False):\n",
    "    d = S.shape[0]\n",
    "    K = np.eye(d, dtype=np.float64)\n",
    "\n",
    "    # Lipschitz for grad(S K S - I + rho K):  ||S||_2^2 + rho\n",
    "    s_norm = np.linalg.norm(S, ord=2)\n",
    "    L = s_norm * s_norm + rho\n",
    "    eta = 0.9 / (L + 1e-12)\n",
    "\n",
    "    g_old = obj_sm(K, S, lam, rho)\n",
    "    for it in range(max_iter):\n",
    "        grad = S @ K @ S - np.eye(d) + rho * K\n",
    "        K_new = K - eta * grad\n",
    "        K_new = soft_threshold_offdiag(K_new, eta * lam)\n",
    "        K_new = 0.5 * (K_new + K_new.T)\n",
    "\n",
    "        rel = np.linalg.norm(K_new - K, ord=\"fro\") / (np.linalg.norm(K, ord=\"fro\") + 1e-12)\n",
    "        K = K_new\n",
    "\n",
    "        if verbose and (it % 25 == 0 or it == max_iter - 1):\n",
    "            g_now = obj_sm(K, S, lam, rho)\n",
    "            print(f\"[SM ] it={it:4d}  obj={g_now:.4f}  relchg={rel:.3e}  step={eta:.2e}\")\n",
    "\n",
    "        if rel < tol:\n",
    "            break\n",
    "\n",
    "    # final SPD projection (precision matrixとして扱うため)\n",
    "    K = project_spd(K, eps=1e-6)\n",
    "    return K\n",
    "\n",
    "# ---------- main loop ----------\n",
    "rows = []\n",
    "for r in range(reps):\n",
    "    seed = 1000 + r\n",
    "    K_true = make_sparse_precision(d, edge_prob=edge_prob, w_scale=w_scale, diag_margin=diag_margin, seed=seed)\n",
    "    X = sample_gaussian(n, K_true, seed=seed + 999)\n",
    "    X = X - X.mean(axis=0, keepdims=True)\n",
    "    S = (X.T @ X) / n\n",
    "\n",
    "    # ---- MLE ----\n",
    "    t0 = time.perf_counter()\n",
    "    K_mle = solve_mle_prox(S, lam=lam, rho=rho, max_iter=max_iter_mle, tol=tol_mle, verbose=False)\n",
    "    t1 = time.perf_counter()\n",
    "    ct_mle = t1 - t0\n",
    "    rmse_mle = rmse_matrix(K_mle, K_true)\n",
    "\n",
    "    # ---- SM ----\n",
    "    t0 = time.perf_counter()\n",
    "    K_sm = solve_sm_prox(S, lam=lam, rho=rho, max_iter=max_iter_sm, tol=tol_sm, verbose=False)\n",
    "    t1 = time.perf_counter()\n",
    "    ct_sm = t1 - t0\n",
    "    rmse_sm = rmse_matrix(K_sm, K_true)\n",
    "\n",
    "    rows.append(dict(rep=r, RMSE_MLE=rmse_mle, CT_MLE=ct_mle, RMSE_SM=rmse_sm, CT_SM=ct_sm))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "summ = df.drop(columns=[\"rep\"]).agg([\"mean\",\"std\"])\n",
    "speedup = summ.loc[\"mean\",\"CT_MLE\"] / summ.loc[\"mean\",\"CT_SM\"]\n",
    "\n",
    "print(\"===== SUMMARY (mean ± std) =====\")\n",
    "print(summ)\n",
    "print(f\"\\nCT ratio (MLE/SM) = {speedup:.2f}x\")\n",
    "\n",
    "# ---------- LaTeX table (book-friendly) ----------\n",
    "m = summ.loc[\"mean\"]\n",
    "s = summ.loc[\"std\"]\n",
    "\n",
    "latex = rf\"\"\"\n",
    "\\begin{{table}}[t]\n",
    "\\centering\n",
    "\\caption{{反復{reps}の推定精度（RMSE）と計算時間（CT）．$d={d},n={n}$．同一の正則化（$\\lambda={lam}$，$\\rho={rho}$）を MLE と SM に課した．}}\n",
    "\\label{{tab:ggm_sum}}\n",
    "\\begin{{tabular}}{{lcc}}\n",
    "\\toprule\n",
    " & RMSE（mean $\\pm$ sd） & CT（sec，mean $\\pm$ sd） \\\\\n",
    "\\midrule\n",
    "MLE &\n",
    "{m['RMSE_MLE']:.6f} $\\pm$ {s['RMSE_MLE']:.6f} &\n",
    "{m['CT_MLE']:.6f} $\\pm$ {s['CT_MLE']:.6f} \\\\\n",
    "SM  &\n",
    "{m['RMSE_SM']:.6f} $\\pm$ {s['RMSE_SM']:.6f} &\n",
    "{m['CT_SM']:.6f} $\\pm$ {s['CT_SM']:.6f} \\\\\n",
    "\\midrule\n",
    "CT比（MLE/SM） & \\multicolumn{{2}}{{c}}{{{speedup:.2f}\\,倍}} \\\\\n",
    "\\bottomrule\n",
    "\\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\"\"\"\n",
    "print(\"\\n===== LaTeX =====\")\n",
    "print(latex)\n",
    "\n",
    "# ---------- knobs (if you want clearer trade-off) ----------\n",
    "print(\"\\nTips:\")\n",
    "print(\" - If RMSE order is not 'MLE < SM', try: increase max_iter_mle, or decrease max_iter_sm, or slightly increase lam for SM only.\")\n",
    "print(\" - If CT ratio is not ~10x, try: increase max_iter_mle (e.g. 400) and decrease max_iter_sm (e.g. 50).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "wGZGy5SnZK7D"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}